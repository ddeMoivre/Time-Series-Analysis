{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis of the US Treasury 10- Year Yield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR(p)  Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series model for the observed data $\\{x_t\\}$ is a specification of the joint distribution (or possibly only the means and covariances) of a sequence of random variables $\\{X_t\\}$ of which $\\{x_t\\}$ is postulated to be a realization. \n",
    "\n",
    "The causal autoregressive $AR(p)$ process is defined by \n",
    "$$\n",
    "X_t-\\phi_1 X_{t-1} - ...-\\phi_p X_{t-p}=c + Z_t, \\ \\ {Z_t} \\sim WN(0,\\sigma^2).\n",
    "$$\n",
    "\n",
    "A time series $\\{X_t\\}$ is (covariance) **stationnary** if the mean function $\\mu_X(t):= E(X_t)$ is independent of $t$, and the autocovariance function (ACVF) of $\\{X_t\\}$ at lag $h$ \n",
    "\n",
    "$$\n",
    "\\gamma_X(t+h,t) := Cov(X_{t+h},X_t) = \\mathbb{E}[(X_{t+h}-\\mu_X(t+h))(X_{t}-\\mu_X(t))]\n",
    "$$\n",
    "\n",
    "is independent of $t$ for each $h$.\n",
    "\n",
    "To assess the degree of dependence in the data and to select a model for the data that reflects this, one of the important tools we use is the sample autocorrelation function (sample ACF) of the data. Let $\\{X_t\\}$ be a stationary time series. The **autocorrelation function** of $\\{X_t\\}$ at lag $h$ is \n",
    "\n",
    "$$\n",
    "\\rho_X(h):=\\frac{\\gamma_X(h)}{\\gamma_X(0)} = Cor(X_{t+h},X_{t}).\n",
    "$$\n",
    "\n",
    "Let $x_1,...,x_n$ be observations of a time series. The **sample autocorrelation function** is \n",
    "\n",
    "$$\n",
    "\\hat\\rho(h) = \\frac{\\hat\\gamma(h)}{\\hat\\gamma(0)},\n",
    "$$\n",
    "\n",
    "where $\\hat\\gamma(h)$ is the sample autocovariance function i.e., $\\hat\\gamma(h): = n^{-1}\\sum_{t=1}^{n-|h|}(x_{n-|h|}-\\bar{x})(x_t-\\bar{x})$, for $\\ -n<h<n$ and $\\bar{x}:=n^{-1}\\sum^n_{t=1} x_t$.\n",
    "\n",
    "We define sample PACF in an analogous way. If we believe that the data are realized values of a stationary time series $\\{X_t\\}$, then the sample ACF will provide us with an estimate of the ACF of $\\{X_t\\}$. This estimate may suggest which of the many possible stationary time series models is a suitable candidate for representing the dependence in the data. For example, a sample ACF that is close to zero for all nonzero lags suggests that an appropriate model for the data might be iid noise.\n",
    "\n",
    "A **partial autocorrelation function (PACF)** of an ARMA process $\\{X_t\\}$ is the function $\\alpha(\\cdot)$ defined by the equations \n",
    "\n",
    "$$\n",
    "\\alpha(0) = 1 \\ \\ \\text{and} \\ \\ \\alpha(h) = \\phi_{hh}, \\ \\ h \\geq 1,\n",
    "$$\n",
    "\n",
    "where $\\phi_{hh}$ is the last component of $\\Phi_h = \\Gamma^{-1}_{h}\\gamma_h$, $\\Gamma_h$ is $h$-dimensional the covariance matrix and $\\gamma_h = [\\gamma(1),\\gamma(2),...,\\gamma(h)]'$.\n",
    "\n",
    "The partial autocorrelation function is a tool that exploits the fact that, whereas an $AR(p)$ process has an autocorrelation function that is infinite in extent, the partial autocorrelations are zero beyond lag $p$. We define sample PACF for observed data in an analogous way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load python libraries and Federal Reserve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands re-load the data and evaluates the presence and nature of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.tsa.ar_model import AutoReg, ar_select_order\n",
    "from pylab import mpl, plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['font.family'] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_data = pd.read_csv(\"fred_data.csv\", index_col=\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fred_data.plot(figsize=(10,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise U.S. Treasury Securities at 10-Year Constant Maturity\n",
    "fig = px.line(fred_data, y='DGS10')\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are dates with missing values (NAs)\n",
    "# Print out the counts of missing values\n",
    "\n",
    "fred_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows for which DGS10 data is missing\n",
    "fred_data[fred_data.isna()[\"DGS10\"]==True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the FRED data is missing when there are holidays or market-closes\n",
    "# in the bond market of the US.\n",
    "\n",
    "# Define fred_data0 as sub matrix with nonmissing data for DGS10\n",
    "fred_data0 = fred_data[fred_data.isna()[\"DGS10\"]==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_data0.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our focus is on DGS10, the yield of constant-maturity 10 Year US bond.\n",
    "DGS10_daily = fred_data0[[\"DGS10\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DGS10_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DGS10_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weekly and monthly time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function resample_plot() converts a time series data object\n",
    "# to an Open/High/Low/Close series on a periodicity lower than the input data object.\n",
    "# Plot OHLC chart which shows the open, high, low, and close price for a given period.\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DGS10_daily['Date'] = pd.to_datetime(DGS10_daily.index, format='%Y/%m/%d')\n",
    "DGS10_daily = DGS10_daily.set_index(['Date'])\n",
    "DGS10_daily.columns = ['DGS10_daily']\n",
    "\n",
    "def resample_plot(data, how):\n",
    "    df = pd.DataFrame(columns=['open', 'high', 'low', 'close'])\n",
    "    ohlc = {'open': lambda x: x[0],\n",
    "            'high': max,\n",
    "            'low': min,\n",
    "            'close': lambda x: x[-1]}\n",
    "    for key in ohlc.keys():\n",
    "        df[key] = data.resample(how).apply(ohlc[key])\n",
    "    fig = go.Figure(data=[go.Candlestick(x=df.index,\n",
    "                open=df.loc[:,'open'], high=df.loc[:,'high'],\n",
    "                low=df.loc[:,'low'], close=df.loc[:,'close'])\n",
    "                     ])\n",
    "    f = lambda x: 'week' if x=='W' else 'month'\n",
    "    fig.update_layout(title='OHLC chart for {}'.format(f(how)),\n",
    "    yaxis_title='DGS10',\n",
    "        xaxis_rangeslider_visible=False)\n",
    "    fig.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OHLC Chart for week\n",
    "OHLC_weekly = resample_plot(DGS10_daily,'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OHLC Chart for month\n",
    "OHLC_monthly = resample_plot(DGS10_daily,'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two vector time series of weekly close values and of monthly close values\n",
    "DGS10_weekly = OHLC_weekly[['close']]\n",
    "DGS10_weekly.columns = ['DGS10_weekly']\n",
    "DGS10_monthly = OHLC_monthly[['close']]\n",
    "DGS10_monthly.columns = ['DGS10_monthly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the dimensions when daily data reduced to weekly and to monthly periods\n",
    "len(DGS10_weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DGS10_monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ACF and PACF for daily, weekly, monthly series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ACF (auto-correlation function) and PACF (partial auto-correlation function) for each periodicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acf_pacf_plot(daily,weekly,monthly):\n",
    "    fig, ax = plt.subplots(2,3, figsize=(16,10))\n",
    "    sm.graphics.tsa.plot_acf(daily.values.squeeze(), \n",
    "                             title = list(daily.columns)[0], ax=ax[0,0])\n",
    "    ax[0,0].set_ylabel('ACF')\n",
    "    sm.graphics.tsa.plot_acf(weekly.values.squeeze(), \n",
    "                             title = list(weekly.columns)[0], ax=ax[0,1])\n",
    "    ax[0,1].set_ylabel('ACF')\n",
    "    sm.graphics.tsa.plot_acf(monthly.values.squeeze(), \n",
    "                             title = list(monthly.columns)[0], ax=ax[0,2])\n",
    "    ax[0,2].set_ylabel('ACF')\n",
    "    sm.graphics.tsa.plot_pacf(daily.values.squeeze(), \n",
    "                             title = list(daily.columns)[0], ax=ax[1,0])\n",
    "    ax[1,0].set_ylabel('Partial ACF')\n",
    "    sm.graphics.tsa.plot_pacf(weekly.values.squeeze(), \n",
    "                             title = list(weekly.columns)[0], ax=ax[1,1])\n",
    "    ax[1,1].set_ylabel('Partial ACF')\n",
    "    sm.graphics.tsa.plot_pacf(monthly.values.squeeze(), \n",
    "                             title = list(monthly.columns)[0], ax=ax[1,2])\n",
    "    ax[1,2].set_ylabel('Partial ACF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_pacf_plot(DGS10_daily, DGS10_weekly, DGS10_monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high first-order auto-correlation suggests that the time series has a unit root on every periodicity (daily, weekly and monthly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct Augmented Dickey-Fuller Test for Unit Roots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is essential to determine whether the time series is \"stationary\". Informally, stationarity is when the auto-covariance is independent of time. Failure to establish stationarity will almost certainly lead to misinterpretation of model identification and diagnostics tests. \n",
    "\n",
    "We perform an Augmented Dickey-Fuller test to establish stationarity under the assumption that the time series has a constant bias but does not exhibit a time trend. In other words, we assume that the time series is already de-trended. \n",
    "\n",
    "If the stationarity test fails, even after first de-trending the time series, then one potential recourse is to simply take differences of time series and predict $\\Delta y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each periodicity, apply the function adfuller() twice:\n",
    "-  to the un-differenced series (null hypothesis: input series has a unit root)\n",
    "-  to the first-differenced series (same null hypothesis about differenced series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the un-differenced series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(DGS10_daily)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(DGS10_weekly)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(DGS10_monthly)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each periodicity, the null hypothesis of a unit root for the time series DGS10 is not rejected at the 0.05 level. The p-value for each test does not fall below standard critical values of 0.05 or 0.01.\n",
    "The p-value is the probability (assuming the null hypothesis is true) of realizing a test statistic as extreme as that computed for the input series. Smaller values (i.e., lower probabilities) provide stronger evidence against the null hyptohesis.\n",
    "The p-value decreases as the periodicity of the data shortens. This suggests that the time-series structure in the series DGS10 may be stronger at higher frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the first-differenced series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(\n",
    "    (DGS10_daily.shift(1)-DGS10_daily).dropna())\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(\n",
    "    (DGS10_weekly.shift(1)-DGS10_weekly).dropna())\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf, p, usedlag, nobs, cvs, aic = sm.tsa.stattools.adfuller(\n",
    "    (DGS10_monthly.shift(1)-DGS10_monthly).dropna())\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the three time periodicities, the ADF test rejects the null hypothesis that a unit root is present for the first-differenced series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ACF and PACF for the differenced series of each periodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One application of the operator $(1 − B)$ produces a new series $\\{Y_t\\}$ with no obvious deviations from stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_DGS10_daily = (DGS10_daily.shift(1)-DGS10_daily).dropna()\n",
    "diff_DGS10_daily.columns = ['diff_DGS10_daily']\n",
    "diff_DGS10_weekly = (DGS10_weekly.shift(1)-DGS10_weekly).dropna()\n",
    "diff_DGS10_weekly.columns = ['diff_DGS10_weekly']\n",
    "diff_DGS10_monthly = (DGS10_monthly.shift(1)-DGS10_monthly).dropna()\n",
    "diff_DGS10_monthly.columns = ['diff_DGS10_monthly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acf_pacf_plot(diff_DGS10_daily, diff_DGS10_weekly, diff_DGS10_monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The apparent time series structure of DGS10 varies with the periodicity:\n",
    "\n",
    "Daily:\n",
    "\n",
    "    strong negative order-7 autocorrelation and partial autocorrelation\n",
    "    strong positive order-30 autocorrelation and partial autocorrelation\n",
    "\n",
    "Weekly:\n",
    "\n",
    "    strong negative order-1 autocorrelation and partial autocorrelation\n",
    "    strong positive order-26 autocorrelation and partial autocorrelation\n",
    "    \n",
    "Monthly:\n",
    "\n",
    "    strong negative order-19 autocorrelation and partial autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0 = px.line(DGS10_monthly, y='DGS10_monthly', height=400)\n",
    "fig0.show();\n",
    "fig1 = px.line(diff_DGS10_monthly, y='diff_DGS10_monthly', height=300)\n",
    "fig1.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differenced series diff_DGS10_monthly crosses the level 0.0 many times over the historical period. There does not appear to be a tendency for the differenced series to stay below (or above) the zero level. The series appears consistent with covariance-stationary time series structure but whether the structure is other than white noise can be evaluated by evaluating AR(p) models for p = 0, 1, 2, ... and determining whether an AR(p) model for p > 0 is identified as better than an AR(0), i.e., white noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best AR(p) model for monthly data using the AIC criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Define the d and q parameters to take any value between 0 and 1\n",
    "p = range(0, 25)\n",
    "\n",
    "AIC = []\n",
    "AR_model = []\n",
    "for param in p:\n",
    "    try:\n",
    "        model = AutoReg(diff_DGS10_monthly.values, param, old_names=False)\n",
    "        results = model.fit()\n",
    "        print('AR({}) - AIC:{}'.format(param, results.aic), end='\\r')\n",
    "        AIC.append(results.aic)\n",
    "        AR_model.append([param])\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('The smallest AIC is {} for model AR({})'.format(min(AIC), \n",
    "            AR_model[AIC.index(min(AIC))][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit this model\n",
    "model = AutoReg(diff_DGS10_monthly.values, 0, old_names=False)\n",
    "\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(lags=40, figsize=(20, 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plots above, we can observe that the residuals are uncorrelated (bottom right plot) and do not exhibit any obvious seasonality (the top left plot). Also, the residuals and roughly normally distributed with zero mean (top right plot). The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) roghly follows the linear trend of samples taken from a standard normal distribution with $N(0, 1)$. Again, this is a strong indication that the residuals are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclud that the best model for differenced data is AR(0), i.e., white noise. Thus for the original data the model is $X_t = 0.0048 + X_{t-1}+Z_t$, where $Z_t \\sim WN(0,\\sigma^2)$. The parameter $\\sigma^2$ may be estimated by equating the sample ACVF with the model ACVF at lag 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm.tsa.stattools.acovf(diff_DGS10_monthly.values, nlag=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the approximate solution $\\sigma^2 = 0.04$, we obtain the following model\n",
    "\n",
    "$$\n",
    "X_t = 0.0048 + X_{t-1}+Z_t, \\ \\ Z_t \\sim WN(0,0.04).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best AR(p) model for weekly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Define the p parameter to take any value between 0 and 25\n",
    "p = range(0, 25)\n",
    "\n",
    "AIC = []\n",
    "AR_model = []\n",
    "for param in p:\n",
    "    try:\n",
    "        model = AutoReg(diff_DGS10_weekly.values, param, old_names=False)\n",
    "        results = model.fit()\n",
    "        print('AR({}) - AIC:{}'.format(param, results.aic), end='\\r')\n",
    "        AIC.append(results.aic)\n",
    "        AR_model.append([param])\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The smallest AIC is {} for model AR({})'\n",
    "      .format(min(AIC), AR_model[AIC.index(min(AIC))][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit this model\n",
    "model = AutoReg(diff_DGS10_weekly.values, 2, old_names=False)\n",
    "\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.plot_diagnostics(lags=40, figsize=(20, 14));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are consistent with their expected behavior under the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the stationarity and cyclicality of the fitted AR(2) model to weekly data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the stationarity we have to show that all roots of characteristic polynomial lie outside the unit circle\n",
    "\n",
    "$$\n",
    "\\phi(z) = 1-\\phi_1 z-\\phi_2 z^2 \\neq 0 \\ \\ \\text{for all} \\ |z|=1.\n",
    "$$\n",
    "\n",
    "From summarize of the Auto Regression model results we have estimates $\\hat\\phi_1 = -0.1$ and $\\hat\\phi_2 = 0.04$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.polyroots() method is used to compute the roots of a polynomial \n",
    "\n",
    "np.polynomial.polynomial.polyroots((1,-0.1,0.04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both roots are complex located outside the unit circle, we conclud that the fitted model is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With complex roots, there is evidence of cyclicality in the series\n",
    "# The following computation computes the period as it is determined by the\n",
    "# coefficients of the characteristic polynomial.\n",
    "\n",
    "twopif=np.arccos( abs(results.params[1])/(2*np.sqrt(results.params[2])))\n",
    "f=twopif/(8*np.arctan(1))\n",
    "period=-1/f\n",
    "print(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data are consistent with cycle of period just over 5 weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Yule–Walker estimator for $\\sigma^2$:\n",
    " $$\n",
    " \\hat\\sigma^2 = \\hat\\gamma(0)-\\hat\\phi_1\\hat\\gamma(1)-\\hat\\phi_2\\hat\\gamma(2)\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variance, first and second autocovariances:', \n",
    "      sm.tsa.stattools.acovf(diff_DGS10_weekly.values, nlag=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation gives\n",
    "# sigma^2 = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we conclude \n",
    "\n",
    "$$\n",
    "Y_t =0.001 -0.1Y_{t-1}+0.04Y_{t-2} + Z_t,\n",
    "$$\n",
    "\n",
    "and for weekly time series\n",
    "\n",
    "$$\n",
    "(1+0.1B-0.04B^2)(1-B)X_t =0.001 + Z_{t}, \\ \\ \\ Z_t \\sim WN(0,0.01).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
